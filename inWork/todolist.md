- [ ] Multi SparkContext Demo

  - [ ] we want to have different configuration for different kind of jobs: 
    e.g.  spark.sql.shuffle.partitions=200 , big-data-job / 10, small-data-job 
  - [ ] http://livy.incubator.apache.org./
  - [ ]  does multi sparkSession can satisfy our need
  - [ ]  when to use which sparkcontext? how to manager? do we need share objects across the contexts?
- [ ] Spark Stage partitions merge
  - [ ] https://databricks.com/session/an-adaptive-execution-engine-for-apache-spark-sql
- [ ] https://www.snappydata.io/
- [ ] Datastax
- [ ] Parquet
- [ ] Transmitter Code Refactor
- [ ] Spark shuffle Read
- [ ] MTDI / Project schema 
  - [ ] SQL Generate

